import os
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from scipy.stats import pearsonr
import json
import sys

# ====================================================================
# CONFIGURATION & FILE PATHS
# ====================================================================
# Ensure these paths match your environment
DATA_DIR = r"C:\Users\jayan\ML_Projects\speech_based_machine_learning_models_of_depression\features"
SENTENCE_LEVEL_PKL = os.path.join(DATA_DIR, "language_features_sentence_level.pkl")
MODEL_SAVE_DIR = r"C:\Users\jayan\ML_Projects\speech_based_machine_learning_models_of_depression\models"

# Files generated by the training script
BEST_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, "contextual_lstm_best_overall.pt")
BEST_HYPERPARAM_PATH = os.path.join(MODEL_SAVE_DIR, "best_language_contextual_hyperparameters.json")

# Output file for the final test results
FINAL_TEST_RESULTS_PATH = os.path.join(MODEL_SAVE_DIR, "language_final_test_results.json")

# Fixed experiment settings (MUST match training script)
RANDOM_STATE = 42
FINAL_TEST_SIZE = 14
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Random seeds for consistency
torch.manual_seed(RANDOM_STATE)
np.random.seed(RANDOM_STATE)


# ====================================================================
# 1) DATA UTILITIES
# ====================================================================
def load_all_features():
	"""Loads features and returns all data structures needed."""
	with open(SENTENCE_LEVEL_PKL, "rb") as f:
		data = pickle.load(f)
	
	participant_ids = data["participant_ids"]
	phq_scores = np.array(data["phq_scores"], dtype=float)
	bert_sequences = data["bert_sequences"]
	sentiment_sequences = data["sentiment_sequences"]
	sequence_lengths = data.get("sequence_lengths", {pid: len(bert_sequences[pid]) for pid in participant_ids})

	example_bert_seq = bert_sequences[participant_ids[0]]
	bert_dim = len(example_bert_seq[0])
	
	ordered_bert_seqs = [bert_sequences[pid] for pid in participant_ids]
	ordered_sent_seqs = [sentiment_sequences[pid] for pid in participant_ids]
	ordered_lengths = [sequence_lengths[pid] for pid in participant_ids]

	return participant_ids, ordered_bert_seqs, ordered_sent_seqs, ordered_lengths, phq_scores, bert_dim


class SentenceLevelDataset(Dataset):
	"""Dataset for sentence-level features."""
	def __init__(self, bert_seqs, sent_seqs, lengths, labels):
		self.bert_seqs = bert_seqs
		self.sent_seqs = sent_seqs
		self.lengths = lengths
		self.labels = labels.astype(float)

	def __len__(self):
		return len(self.labels)

	def __getitem__(self, idx):
		bert_seq = np.stack(self.bert_seqs[idx]).astype(np.float32) if isinstance(self.bert_seqs[idx], list) else self.bert_seqs[idx].astype(np.float32)
		sent_seq = np.array(self.sent_seqs[idx], dtype=np.float32)
		length = int(self.lengths[idx])
		label = float(self.labels[idx])
		return bert_seq, sent_seq, length, label


def collate_fn(batch):
	"""Pads sequences and computes aggregated sentiment features."""
	bert_seqs, sent_seqs, lengths, labels = zip(*batch)
	lengths = np.array(lengths, dtype=np.int64)
	max_len = int(lengths.max())

	batch_size = len(batch)
	bert_dim_local = bert_seqs[0].shape[1]
	
	bert_padded = np.zeros((batch_size, max_len, bert_dim_local), dtype=np.float32)
	sent_padded = np.zeros((batch_size, max_len), dtype=np.float32)
	mask = np.zeros((batch_size, max_len), dtype=np.float32)

	for i, (b_seq, s_seq, L) in enumerate(zip(bert_seqs, sent_seqs, lengths)):
		bert_padded[i, :L, :] = b_seq
		sent_padded[i, :L] = s_seq
		mask[i, :L] = 1.0

	bert_padded = torch.from_numpy(bert_padded)
	sent_padded = torch.from_numpy(sent_padded).unsqueeze(-1)
	mask = torch.from_numpy(mask)
	lengths_tensor = torch.from_numpy(lengths)
	labels_tensor = torch.tensor(labels, dtype=torch.float32)

	mask_sum = mask.sum(dim=1)
	sent_mean = (sent_padded.squeeze(-1) * mask).sum(dim=1) / (mask_sum + 1e-8)
	squared_diff = (sent_padded.squeeze(-1) - sent_mean.unsqueeze(1))**2 * mask
	sent_std = torch.sqrt(squared_diff.sum(dim=1) / (mask_sum + 1e-8))

	sent_agg = torch.stack([sent_mean, sent_std], dim=1)  # (B, 2)

	return bert_padded, sent_padded, mask, lengths_tensor, sent_agg, labels_tensor


# ====================================================================
# 2) MODEL ARCHITECTURE
# ====================================================================
class ContextualLSTM(nn.Module):
	"""
	Unified LSTM model with different fusion mechanisms for BERT and VADER sentiment.
	"""
	def __init__(self, bert_dim=384, hidden_size=128, num_layers=2, dropout=0.3, fusion="concat"):
		super().__init__()
		assert fusion in ("concat", "gated", "attention")
		self.fusion = fusion
		self.hidden_size = hidden_size
		self.num_layers = num_layers
		self.dropout = dropout

		if fusion == "concat":
			input_size = bert_dim + 1
		else:
			input_size = bert_dim

		self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
							num_layers=num_layers, batch_first=True,
							dropout=dropout if num_layers > 1 else 0)

		if fusion in ("gated", "attention"):
			self.sent_agg_encoder = nn.Sequential(
				nn.Linear(2, 32), nn.ReLU(), nn.Dropout(dropout), nn.Linear(32, hidden_size)
			)

		if fusion == "gated":
			self.gate_layer = nn.Linear(hidden_size, hidden_size)

		if fusion == "attention":
			self.att_text = nn.Linear(hidden_size, hidden_size)
			self.att_sent = nn.Linear(hidden_size, hidden_size)
			self.att_comb = nn.Linear(hidden_size, 1)

		self.fc1 = nn.Linear(hidden_size, 64)
		self.fc2 = nn.Linear(64, 1)
		self.relu = nn.ReLU()
		self.drop = nn.Dropout(dropout)

	def forward(self, bert_seq, sent_seq, mask, lengths, sent_agg):
		B, T, D = bert_seq.shape
		
		if self.fusion == "concat":
			x = torch.cat([bert_seq, sent_seq], dim=2)
		else:
			x = bert_seq
		
		packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)
		packed_out, (h_n, c_n) = self.lstm(packed)
		
		if self.fusion == "concat":
			h = h_n[-1]
		else:
			lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=T)
			
			# Get the last valid output state
			idx = (lengths - 1).long().unsqueeze(1).unsqueeze(2).expand(B, 1, self.hidden_size)
			h_last = lstm_out.gather(1, idx).squeeze(1)
			
			if self.fusion == "gated":
				sent_vec = self.sent_agg_encoder(sent_agg)
				gate = torch.sigmoid(self.gate_layer(sent_vec))
				h = h_last * gate
			else: # attention
				sent_vec = self.sent_agg_encoder(sent_agg)
				sent_vec_exp = sent_vec.unsqueeze(1)
				
				text_proj = self.att_text(lstm_out)
				sent_proj = self.att_sent(sent_vec_exp)
				att_in = torch.tanh(text_proj + sent_proj)
				att_scores = self.att_comb(att_in).squeeze(-1)
				
				att_scores = att_scores.masked_fill(mask == 0, float("-inf"))
				att_weights = F.softmax(att_scores, dim=1).unsqueeze(-1)
				h = torch.sum(lstm_out * att_weights, dim=1)

		out = self.fc1(h)
		out = self.relu(out)
		out = self.drop(out)
		out = self.fc2(out).squeeze(1)
		return out


# ====================================================================
# 3) EVALUATION FUNCTION
# ====================================================================
def eval_model(model, loader, device):
	"""Evaluates the model and computes metrics."""
	model.eval()
	preds_all = []
	labels_all = []
	with torch.no_grad():
		for bert_p, sent_p, mask, lengths, sent_agg, labels in loader:
			bert_p = bert_p.to(device)
			sent_p = sent_p.to(device)
			mask = mask.to(device)
			lengths = lengths.to(device)
			sent_agg = sent_agg.to(device)

			preds = model(bert_p, sent_p, mask, lengths, sent_agg)
			preds_all.append(preds.cpu().numpy())
			labels_all.append(labels.numpy())

	preds_all = np.concatenate(preds_all)
	labels_all = np.concatenate(labels_all)

	if np.std(preds_all) == 0 or np.std(labels_all) == 0:
		r = 0.0
	else:
		r, _ = pearsonr(labels_all, preds_all)

	max_label = np.max(labels_all) if np.max(labels_all) != 0 else 1.0
	re = np.mean(np.abs(preds_all - labels_all) / max_label)

	return r, re, preds_all, labels_all


# ====================================================================
# 4) MAIN EXECUTION BLOCK - FINAL TEST ONLY
# ====================================================================
if __name__ == "__main__":
	print("Starting Language Model Final Test Evaluation...")
	
	# --- 4.1 Load Data and Apply Fixed Split ---
	try:
		(all_pids, all_bert_seqs, all_sent_seqs, all_lengths, all_phq, bert_dim) = load_all_features()
	except FileNotFoundError as e:
		print(f"Error: {e}")
		sys.exit(1)

	all_indices = np.arange(len(all_pids))
	
	# This split ensures we select the SAME 14 PIDs as the training script
	train_cv_idx, final_test_idx, _, _ = train_test_split(
		all_indices, all_indices, 
		test_size=FINAL_TEST_SIZE, 
		random_state=RANDOM_STATE
	)

	# Prepare Test Data
	pids_test = [all_pids[i] for i in final_test_idx]
	bert_seqs_test = [all_bert_seqs[i] for i in final_test_idx]
	sent_seqs_test = [all_sent_seqs[i] for i in final_test_idx]
	lengths_test = [all_lengths[i] for i in final_test_idx]
	phq_test = all_phq[final_test_idx]
	
	test_dataset = SentenceLevelDataset(bert_seqs_test, sent_seqs_test, lengths_test, phq_test)
	
	print(f"Identified {len(test_dataset)} participants for final testing (N={FINAL_TEST_SIZE}).")


	# --- 4.2 Load Best Hyperparameters ---
	try:
		with open(BEST_HYPERPARAM_PATH, "r") as f:
			best_hparams = json.load(f)
		print(f"Loaded Best CV Hparams: Fusion={best_hparams['fusion']}, Hidden={best_hparams['hidden_size']}")
	except FileNotFoundError:
		print(f"Error: Hyperparameter file not found at {BEST_HYPERPARAM_PATH}. Run training script first.")
		sys.exit(1)


	# --- 4.3 Initialize Model and Load Weights ---
	final_model = ContextualLSTM(
		bert_dim=bert_dim, 
		hidden_size=best_hparams["hidden_size"],
		num_layers=best_hparams["num_layers"],
		dropout=best_hparams["dropout"], 
		fusion=best_hparams["fusion"]
	).to(DEVICE)
	
	try:
		final_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))
		print(f"Loaded model weights from: {BEST_MODEL_PATH}")
	except FileNotFoundError:
		print(f"Error: Model weights not found at {BEST_MODEL_PATH}. Run training script first.")
		sys.exit(1)


	# --- 4.4 Run Final Evaluation ---
	test_loader = DataLoader(
		test_dataset, 
		batch_size=best_hparams["batch_size"], 
		shuffle=False, 
		collate_fn=collate_fn
	)

	r_final, re_final, preds_final, trues_final = eval_model(final_model, test_loader, DEVICE)

	# --- 4.5 Report and Save Results ---
	print("\n*** Language Final Holdout Test Performance (N=14) ***")
	print(f"  Pearson's r: **{r_final:.4f}**")
	print(f"  Relative Error (RE): **{re_final:.4f}**")

	final_test_summary = {
		"model": "Language Contextual LSTM",
		"N_test": len(pids_test),
		"r_test": float(r_final) if np.isfinite(r_final) else None,
		"re_test": float(re_final) if np.isfinite(re_final) else None,
		"test_pids": pids_test,
		"true_scores": trues_final.tolist(),
		"predicted_scores": preds_final.tolist(),
		"best_cv_hyperparams": best_hparams
	}
	
	# Save the final results to JSON file
	with open(FINAL_TEST_RESULTS_PATH, "w") as f:
		json.dump(final_test_summary, f, indent=4)
		
	print(f"\nFinal test results saved to: {FINAL_TEST_RESULTS_PATH}")
	print("Done.")