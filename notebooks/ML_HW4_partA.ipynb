{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Statement of AI Assistance:\n",
        "    In the following code for part (a), ChatGPT was used to implement the TF-IDF\n",
        "    vectorizer for the input data, and the sentiment scores.\n",
        "\"\"\"\n",
        "\n",
        "# Project imports\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download Vader for sentiment scores\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "\n",
        "# Load depression labels\n",
        "labels_path = \"/content/sample_data/Depression_Labels/DepressionLabels.xlsx\"\n",
        "\n",
        "df_dplabel = pd.read_excel(labels_path)\n",
        "df_dplabel = df_dplabel.rename(columns={\"Participant_ID\": \"ParticipantID\"})\n",
        "\n",
        "print(\"Depression labels shape:\", df_dplabel.shape)\n",
        "print(df_dplabel.head())\n",
        "\n",
        "\n",
        "# Load transcripts for each participant\n",
        "transcript_paths = sorted(glob.glob(\"/content/sample_data/EDAIC_Transcripts/*.csv\"))\n",
        "\n",
        "dfs_trans = []\n",
        "\n",
        "for path in transcript_paths:\n",
        "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
        "\n",
        "    # Extract participant ID from filename\n",
        "    fname = os.path.basename(path)\n",
        "    pid = int(fname.split(\"_\")[0])\n",
        "    df[\"ParticipantID\"] = pid\n",
        "\n",
        "    dfs_trans.append(df)\n",
        "\n",
        "df_trans_all = pd.concat(dfs_trans, ignore_index=True)\n",
        "\n",
        "\n",
        "# Load acoustics\n",
        "acoustic_paths = sorted(glob.glob(\"/content/sample_data/EDAIC_Acoustics/*.csv\"))\n",
        "\n",
        "dfs_acous = []\n",
        "\n",
        "for path in acoustic_paths:\n",
        "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
        "    dfs_acous.append(df)\n",
        "\n",
        "df_acous_all = pd.concat(dfs_acous, ignore_index=True)\n",
        "\n",
        "\n",
        "# Compute sentiment scores\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "\"\"\"\n",
        "  - sia.polarity_scores(t): returns a dictionary with keys (neg, neu, pos, compound)\n",
        "  - [\"compound\"] extracts the sentiment score in range [-1, 1] (neg to pos)\n",
        "  - sent_scores: pandas series with one sentiment score per utterance\n",
        "\"\"\"\n",
        "df_trans_all[\"sentiment_compound\"] = (\n",
        "    df_trans_all[\"Text\"]\n",
        "    .fillna(\"\")\n",
        "    .apply(lambda t: sia.polarity_scores(t)[\"compound\"])\n",
        ")\n",
        "\n",
        "# Take the mean sentiment score for each participant\n",
        "sentiment_by_participant = (\n",
        "    df_trans_all\n",
        "    .groupby(\"ParticipantID\")[\"sentiment_compound\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Take the mean acoustic feature for each participant\n",
        "acous_features_by_participant = (\n",
        "    df_acous_all\n",
        "    .groupby(\"ParticipantID\")\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Merge sentiment and acoustics features for each participant\n",
        "df_features = (\n",
        "    df_dplabel\n",
        "    .merge(sentiment_by_participant, on=\"ParticipantID\", how=\"inner\")\n",
        "    .merge(acous_features_by_participant, on=\"ParticipantID\", how=\"inner\")\n",
        ")\n",
        "\n",
        "print(\"\\nFinal df_features shape:\", df_features.shape)\n",
        "print(df_features.head())\n",
        "print(\"\\nUnique participants in df_features:\", df_features[\"ParticipantID\"].nunique())\n",
        "\n",
        "\n",
        "# Build data matrix and labels\n",
        "X = df_features.drop(columns=[\"PHQ_Score\", \"ParticipantID\"])\n",
        "y = df_features[\"PHQ_Score\"]\n",
        "\n",
        "print(\"\\nX shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"\\nX columns (first 10):\", X.columns[:10].tolist())\n",
        "print(\"\\ny head:\")\n",
        "print(y.head())\n",
        "\n",
        "\n",
        "# Syntactic vectorizer implementation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Combine all utterances per participant into a single string\n",
        "str_by_participant = (\n",
        "    df_trans_all\n",
        "    .groupby(\"ParticipantID\")[\"Text\"]\n",
        "    .apply(lambda x: \" \".join(x.astype(str)))\n",
        ")\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "tdidf.fit_transform(df_trans_labeled[\"Text\"].fillna(\"\"))\n",
        "  - df_trans_labeled[\"Text\"].fillna(\"\") extracts the text column and replaces\n",
        "    NaN entries with empty strings\n",
        "  - .fit_transform(): learns vocab and IDF weights, and converts text to TF-IDF\n",
        "    feature vector\n",
        "Output\n",
        "  - X_tfidf_participant: matrix of shape (N_utterances, 5000) where each row is an utterance,\n",
        "    each column is a TD-IDF feature\n",
        "\"\"\"\n",
        "X_tfidf_participant = tfidf.fit_transform(str_by_participant)\n",
        "\n",
        "print(\"\\nX_tfidf_participant shape (participants x vocab):\", X_tfidf_participant.shape)\n",
        "# docs_by_participant.index is the ParticipantID index order that matches rows in X_tfidf_participant\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDeC4TURdWxI",
        "outputId": "654c0bcc-18f8-4d94-cb1f-4c0b6beb982f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depression labels shape: (219, 2)\n",
            "   ParticipantID  PHQ_Score\n",
            "0            300          2\n",
            "1            301          3\n",
            "2            302          4\n",
            "3            303          0\n",
            "4            304          6\n",
            "\n",
            "Final df_features shape: (134, 30)\n",
            "   ParticipantID  PHQ_Score  sentiment_compound  UtteranceIndex  Start_Time  \\\n",
            "0            386         11            0.223593            41.0  519.309877   \n",
            "1            387          2            0.215683            46.5  287.426087   \n",
            "2            388         17            0.040740            49.5  407.325510   \n",
            "3            389         14            0.047293            59.5  468.931356   \n",
            "4            390          9            0.121897            77.0  707.501961   \n",
            "\n",
            "     End_Time  Confidence  Loudness_sma3  alphaRatio_sma3  \\\n",
            "0  541.035802    0.933917       0.147390       -15.719342   \n",
            "1  289.713043    0.931755       0.094281       -17.290294   \n",
            "2  409.063265    0.888103       0.103571       -16.480069   \n",
            "3  470.710169    0.897373       0.095695       -20.799213   \n",
            "4  711.226144    0.900761       0.106446       -17.288813   \n",
            "\n",
            "   hammarbergIndex_sma3  ...  HNRdBACF_sma3nz  logRelF0-H1-H2_sma3nz  \\\n",
            "0             26.607891  ...         3.482473               1.909257   \n",
            "1             27.380149  ...         1.723084               1.964119   \n",
            "2             26.983015  ...         0.816275               0.918457   \n",
            "3             31.428066  ...         1.758241               1.157377   \n",
            "4             29.355163  ...         1.667827               1.331202   \n",
            "\n",
            "   logRelF0-H1-A3_sma3nz  F1frequency_sma3nz  F1bandwidth_sma3nz  \\\n",
            "0              10.359682          634.552263         1405.294913   \n",
            "1               9.019465          572.281242         1351.998246   \n",
            "2               6.082720          594.451010         1364.159019   \n",
            "3               9.463870          599.479117         1391.206038   \n",
            "4               9.926747          565.446810         1398.795676   \n",
            "\n",
            "   F1amplitudeLogRelF0_sma3nz  F2frequency_sma3nz  F2amplitudeLogRelF0_sma3nz  \\\n",
            "0                 -122.744935         1623.971687                 -128.087978   \n",
            "1                 -135.052415         1551.039093                 -140.359126   \n",
            "2                 -153.520975         1571.977236                 -156.991100   \n",
            "3                 -138.436783         1588.695327                 -143.443426   \n",
            "4                 -134.936629         1522.176258                 -139.494192   \n",
            "\n",
            "   F3frequency_sma3nz  F3amplitudeLogRelF0_sma3nz  \n",
            "0         2584.433064                 -131.009492  \n",
            "1         2525.715990                 -142.535582  \n",
            "2         2549.702244                 -158.777582  \n",
            "3         2549.110120                 -145.861499  \n",
            "4         2450.182795                 -142.697953  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "\n",
            "Unique participants in df_features: 134\n",
            "\n",
            "X shape: (134, 28)\n",
            "y shape: (134,)\n",
            "\n",
            "X columns (first 10): ['sentiment_compound', 'UtteranceIndex', 'Start_Time', 'End_Time', 'Confidence', 'Loudness_sma3', 'alphaRatio_sma3', 'hammarbergIndex_sma3', 'slope0-500_sma3', 'slope500-1500_sma3']\n",
            "\n",
            "y head:\n",
            "0    11\n",
            "1     2\n",
            "2    17\n",
            "3    14\n",
            "4     9\n",
            "Name: PHQ_Score, dtype: int64\n",
            "\n",
            "X_tfidf_participant shape (participants x vocab): (190, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "exu7JoTBtqDE",
        "outputId": "1e5f3ccd-c033-4e61-ef43-55efe0c84b8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSuXeK-4tq-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}